<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Image Classification with Pretrained CNN Model | Eric Chen </title> <meta name="author" content="Eric Chen"> <meta name="description" content="A model built off the MobileNet V3 model on the CIFAR-10/CINIC-10 datasets."> <meta name="keywords" content="computers, programming, chemistry, academic, portfolio"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.css" integrity="sha256-uRX+PiRTR4ysKFRCykT8HLuRCub26LgXJZym3Yeom1c=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico?1da50923312575a5a1a14828b0340b9e"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://echen0719.github.io/projects/img-class-cnn/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Eric Chen </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/resume/">Resume </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Image Classification with Pretrained CNN Model</h1> <p class="post-description">A model built off the MobileNet V3 model on the CIFAR-10/CINIC-10 datasets.</p> </header> <article> <p>Well…what do we see here! Below what you are currently reading is a box containing a few buttons. In case you missed the title, here’s a quick overview. Here, I am hosting a pretrained MobileNet V3 model on CIFAR-10 and CINIC-10 datasets so it can predict an image as either an airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck.</p> <h3 id="instructions">Instructions:</h3> <ul> <li>Upload image from your own computer using the button labeled as “Browse…”</li> <li>Enter a URL linking to an image on the internet and click the button labeled as “Upload”</li> <li>Click the button labeled as “Random Image” to pass an image from a preset folder.</li> <li>Please wait a few seconds for the model to initialize (especially on mobile devices).</li> </ul> <div style="border-style: solid; border-width: 3px; border-radius: 5px; padding: 8px 3px 3px 5px"> <label for="fileInput"><strong>Upload Image:</strong></label> <input id="fileInput" type="file" name="file" accept="image/*" hidden=""> <button type="button" onclick="document.getElementById('fileInput').click()">Browse...</button> <br> <label for="imageUrl"><strong>Or Enter Image URL:</strong></label> <input type="text" id="imageUrl" placeholder="https://somewebsite.com/somepicture.png" style="width: 25%;"> <button id="uploadBtn">Upload</button> <br> <label for="randImg"><strong>Or Pick Random:</strong></label> <button id="randImg">Random Image</button> <br><br> <div style="display: flex; align-items: center;"> <label for="preview" style="margin: 0; padding-right: 1%;"><strong>Preview:</strong></label> <img id="preview"> </div> </div> <div id="output" style="padding: 1em 0;"></div> <div id="barChart"></div> <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script> <script src="https://cdn.plot.ly/plotly-3.0.1.min.js"></script> <script>
// 500 images provided by the CINIC-10 dataset
const imgArr = [
    'cifar10-train-0.png',
    'cifar10-train-1.png',
    'cifar10-train-5.png',
    'cifar10-train-10.png',
    'cifar10-train-11.png',
    'cifar10-train-16.png',
    'cifar10-train-18.png',
    'cifar10-train-19.png',
    'cifar10-train-21.png',
    'cifar10-train-25.png',
    'cifar10-train-30.png',
    'cifar10-train-33.png',
    'cifar10-train-36.png',
    'cifar10-train-39.png',
    'cifar10-train-40.png',
    'cifar10-train-44.png',
    'cifar10-train-48.png',
    'cifar10-train-49.png',
    'cifar10-train-53.png',
    'cifar10-train-58.png',
    'cifar10-train-60.png',
    'cifar10-train-62.png',
    'cifar10-train-63.png',
    'cifar10-train-65.png',
    'cifar10-train-70.png',
    'cifar10-train-71.png',
    'cifar10-train-73.png',
    'cifar10-train-74.png',
    'cifar10-train-78.png',
    'cifar10-train-79.png',
    'cifar10-train-81.png',
    'cifar10-train-82.png',
    'cifar10-train-85.png',
    'cifar10-train-86.png',
    'cifar10-train-87.png',
    'cifar10-train-88.png',
    'cifar10-train-91.png',
    'cifar10-train-93.png',
    'cifar10-train-94.png',
    'cifar10-train-95.png',
    'cifar10-train-105.png',
    'cifar10-train-106.png',
    'cifar10-train-108.png',
    'cifar10-train-110.png',
    'cifar10-train-113.png',
    'cifar10-train-114.png',
    'cifar10-train-121.png',
    'cifar10-train-124.png',
    'cifar10-train-125.png',
    'cifar10-train-127.png',
    'cifar10-train-128.png',
    'cifar10-train-131.png',
    'cifar10-train-135.png',
    'cifar10-train-136.png',
    'cifar10-train-138.png',
    'cifar10-train-140.png',
    'cifar10-train-141.png',
    'cifar10-train-142.png',
    'cifar10-train-146.png',
    'cifar10-train-149.png',
    'cifar10-train-153.png',
    'cifar10-train-155.png',
    'cifar10-train-158.png',
    'cifar10-train-159.png',
    'cifar10-train-160.png',
    'cifar10-train-161.png',
    'cifar10-train-163.png',
    'cifar10-train-167.png',
    'cifar10-train-168.png',
    'cifar10-train-169.png',
    'cifar10-train-170.png',
    'cifar10-train-173.png',
    'cifar10-train-175.png',
    'cifar10-train-177.png',
    'cifar10-train-180.png',
    'cifar10-train-182.png',
    'cifar10-train-183.png',
    'cifar10-train-185.png',
    'cifar10-train-186.png',
    'cifar10-train-190.png',
    'cifar10-train-191.png',
    'cifar10-train-192.png',
    'cifar10-train-194.png',
    'cifar10-train-195.png',
    'cifar10-train-204.png',
    'cifar10-train-206.png',
    'cifar10-train-207.png',
    'cifar10-train-209.png',
    'cifar10-train-213.png',
    'cifar10-train-215.png',
    'cifar10-train-216.png',
    'cifar10-train-218.png',
    'cifar10-train-223.png',
    'cifar10-train-224.png',
    'cifar10-train-226.png',
    'cifar10-train-227.png',
    'cifar10-train-232.png',
    'cifar10-train-233.png',
    'cifar10-train-234.png',
    'cifar10-train-235.png',
    'cifar10-train-238.png',
    'cifar10-train-240.png',
    'cifar10-train-242.png',
    'cifar10-train-247.png',
    'cifar10-train-250.png',
    'cifar10-train-253.png',
    'cifar10-train-256.png',
    'cifar10-train-257.png',
    'cifar10-train-258.png',
    'cifar10-train-260.png',
    'cifar10-train-268.png',
    'cifar10-train-271.png',
    'cifar10-train-273.png',
    'cifar10-train-277.png',
    'cifar10-train-278.png',
    'cifar10-train-281.png',
    'cifar10-train-287.png',
    'cifar10-train-291.png',
    'cifar10-train-293.png',
    'cifar10-train-295.png',
    'cifar10-train-303.png',
    'cifar10-train-309.png',
    'cifar10-train-311.png',
    'cifar10-train-313.png',
    'cifar10-train-314.png',
    'cifar10-train-323.png',
    'cifar10-train-324.png',
    'cifar10-train-326.png',
    'cifar10-train-327.png',
    'cifar10-train-328.png',
    'cifar10-train-332.png',
    'cifar10-train-337.png',
    'cifar10-train-341.png',
    'cifar10-train-342.png',
    'cifar10-train-345.png',
    'cifar10-train-347.png',
    'cifar10-train-348.png',
    'cifar10-train-349.png',
    'cifar10-train-350.png',
    'cifar10-train-356.png',
    'cifar10-train-357.png',
    'cifar10-train-358.png',
    'cifar10-train-362.png',
    'cifar10-train-363.png',
    'cifar10-train-364.png',
    'cifar10-train-365.png',
    'cifar10-train-366.png',
    'cifar10-train-367.png',
    'cifar10-train-369.png',
    'cifar10-train-372.png',
    'cifar10-train-376.png',
    'cifar10-train-380.png',
    'cifar10-train-381.png',
    'cifar10-train-387.png',
    'cifar10-train-389.png',
    'cifar10-train-390.png',
    'cifar10-train-396.png',
    'cifar10-train-397.png',
    'cifar10-train-398.png',
    'cifar10-train-399.png',
    'cifar10-train-402.png',
    'cifar10-train-408.png',
    'cifar10-train-411.png',
    'cifar10-train-412.png',
    'cifar10-train-414.png',
    'cifar10-train-421.png',
    'cifar10-train-422.png',
    'cifar10-train-423.png',
    'cifar10-train-425.png',
    'cifar10-train-428.png',
    'cifar10-train-432.png',
    'cifar10-train-434.png',
    'cifar10-train-436.png',
    'cifar10-train-437.png',
    'cifar10-train-440.png',
    'cifar10-train-447.png',
    'cifar10-train-453.png',
    'cifar10-train-455.png',
    'cifar10-train-456.png',
    'cifar10-train-461.png',
    'cifar10-train-464.png',
    'cifar10-train-466.png',
    'cifar10-train-469.png',
    'cifar10-train-472.png',
    'cifar10-train-474.png',
    'cifar10-train-476.png',
    'cifar10-train-478.png',
    'cifar10-train-481.png',
    'cifar10-train-483.png',
    'cifar10-train-485.png',
    'cifar10-train-488.png',
    'cifar10-train-494.png',
    'cifar10-train-496.png',
    'cifar10-train-497.png',
    'cifar10-train-498.png',
    'cifar10-train-499.png',
    'cifar10-train-505.png',
    'cifar10-train-507.png',
    'cifar10-train-508.png',
    'cifar10-train-509.png',
    'cifar10-train-510.png',
    'cifar10-train-511.png',
    'cifar10-train-514.png',
    'cifar10-train-523.png',
    'cifar10-train-529.png',
    'cifar10-train-534.png',
    'cifar10-train-536.png',
    'cifar10-train-538.png',
    'cifar10-train-539.png',
    'cifar10-train-540.png',
    'cifar10-train-541.png',
    'cifar10-train-542.png',
    'cifar10-train-543.png',
    'cifar10-train-545.png',
    'cifar10-train-547.png',
    'cifar10-train-549.png',
    'cifar10-train-553.png',
    'cifar10-train-560.png',
    'cifar10-train-563.png',
    'cifar10-train-565.png',
    'cifar10-train-566.png',
    'cifar10-train-568.png',
    'cifar10-train-572.png',
    'cifar10-train-574.png',
    'cifar10-train-576.png',
    'cifar10-train-578.png',
    'cifar10-train-579.png',
    'cifar10-train-581.png',
    'cifar10-train-585.png',
    'cifar10-train-588.png',
    'cifar10-train-589.png',
    'cifar10-train-594.png',
    'cifar10-train-596.png',
    'cifar10-train-599.png',
    'cifar10-train-600.png',
    'cifar10-train-601.png',
    'cifar10-train-602.png',
    'cifar10-train-606.png',
    'cifar10-train-607.png',
    'cifar10-train-613.png',
    'cifar10-train-614.png',
    'cifar10-train-616.png',
    'cifar10-train-621.png',
    'cifar10-train-627.png',
    'cifar10-train-630.png',
    'cifar10-train-631.png',
    'cifar10-train-635.png',
    'cifar10-train-638.png',
    'cifar10-train-639.png',
    'cifar10-train-641.png',
    'cifar10-train-645.png',
    'cifar10-train-650.png',
    'cifar10-train-651.png',
    'cifar10-train-652.png',
    'cifar10-train-653.png',
    'cifar10-train-654.png',
    'cifar10-train-656.png',
    'cifar10-train-657.png',
    'cifar10-train-658.png',
    'cifar10-train-667.png',
    'cifar10-train-671.png',
    'cifar10-train-673.png',
    'cifar10-train-681.png',
    'cifar10-train-683.png',
    'cifar10-train-684.png',
    'cifar10-train-685.png',
    'cifar10-train-692.png',
    'cifar10-train-694.png',
    'cifar10-train-698.png',
    'cifar10-train-701.png',
    'cifar10-train-707.png',
    'cifar10-train-709.png',
    'cifar10-train-712.png',
    'cifar10-train-714.png',
    'cifar10-train-716.png',
    'cifar10-train-717.png',
    'cifar10-train-719.png',
    'cifar10-train-720.png',
    'cifar10-train-721.png',
    'cifar10-train-722.png',
    'cifar10-train-728.png',
    'cifar10-train-730.png',
    'cifar10-train-734.png',
    'cifar10-train-735.png',
    'cifar10-train-736.png',
    'cifar10-train-738.png',
    'cifar10-train-744.png',
    'cifar10-train-745.png',
    'cifar10-train-746.png',
    'cifar10-train-747.png',
    'cifar10-train-748.png',
    'cifar10-train-749.png',
    'cifar10-train-751.png',
    'cifar10-train-752.png',
    'cifar10-train-753.png',
    'cifar10-train-754.png',
    'cifar10-train-755.png',
    'cifar10-train-766.png',
    'cifar10-train-768.png',
    'cifar10-train-769.png',
    'cifar10-train-771.png',
    'cifar10-train-772.png',
    'cifar10-train-773.png',
    'cifar10-train-774.png',
    'cifar10-train-776.png',
    'cifar10-train-780.png',
    'cifar10-train-781.png',
    'cifar10-train-783.png',
    'cifar10-train-784.png',
    'cifar10-train-786.png',
    'cifar10-train-793.png',
    'cifar10-train-794.png',
    'cifar10-train-797.png',
    'cifar10-train-798.png',
    'cifar10-train-801.png',
    'cifar10-train-805.png',
    'cifar10-train-807.png',
    'cifar10-train-814.png',
    'cifar10-train-817.png',
    'cifar10-train-825.png',
    'cifar10-train-826.png',
    'cifar10-train-827.png',
    'cifar10-train-831.png',
    'cifar10-train-832.png',
    'cifar10-train-838.png',
    'cifar10-train-839.png',
    'cifar10-train-841.png',
    'cifar10-train-842.png',
    'cifar10-train-843.png',
    'cifar10-train-844.png',
    'cifar10-train-846.png',
    'cifar10-train-848.png',
    'cifar10-train-853.png',
    'cifar10-train-861.png',
    'cifar10-train-862.png',
    'cifar10-train-863.png',
    'cifar10-train-864.png',
    'cifar10-train-865.png',
    'cifar10-train-869.png',
    'cifar10-train-871.png',
    'cifar10-train-873.png',
    'cifar10-train-874.png',
    'cifar10-train-875.png',
    'cifar10-train-877.png',
    'cifar10-train-881.png',
    'cifar10-train-884.png',
    'cifar10-train-888.png',
    'cifar10-train-892.png',
    'cifar10-train-893.png',
    'cifar10-train-896.png',
    'cifar10-train-900.png',
    'cifar10-train-905.png',
    'cifar10-train-907.png',
    'cifar10-train-910.png',
    'cifar10-train-913.png',
    'cifar10-train-916.png',
    'cifar10-train-921.png',
    'cifar10-train-923.png',
    'cifar10-train-924.png',
    'cifar10-train-934.png',
    'cifar10-train-937.png',
    'cifar10-train-939.png',
    'cifar10-train-940.png',
    'cifar10-train-947.png',
    'cifar10-train-952.png',
    'cifar10-train-957.png',
    'cifar10-train-960.png',
    'cifar10-train-962.png',
    'cifar10-train-964.png',
    'cifar10-train-966.png',
    'cifar10-train-970.png',
    'cifar10-train-972.png',
    'cifar10-train-975.png',
    'cifar10-train-977.png',
    'cifar10-train-979.png',
    'cifar10-train-983.png',
    'cifar10-train-984.png',
    'cifar10-train-989.png',
    'cifar10-train-991.png',
    'cifar10-train-992.png',
    'cifar10-train-995.png',
    'cifar10-train-999.png',
    'cifar10-train-1001.png',
    'cifar10-train-1003.png',
    'cifar10-train-1004.png',
    'cifar10-train-1006.png',
    'cifar10-train-1007.png',
    'cifar10-train-1009.png',
    'cifar10-train-1013.png',
    'cifar10-train-1015.png',
    'cifar10-train-1016.png',
    'cifar10-train-1017.png',
    'cifar10-train-1020.png',
    'cifar10-train-1021.png',
    'cifar10-train-1022.png',
    'cifar10-train-1024.png',
    'cifar10-train-1029.png',
    'cifar10-train-1030.png',
    'cifar10-train-1034.png',
    'cifar10-train-1037.png',
    'cifar10-train-1038.png',
    'cifar10-train-1039.png',
    'cifar10-train-1042.png',
    'cifar10-train-1043.png',
    'cifar10-train-1044.png',
    'cifar10-train-1047.png',
    'cifar10-train-1049.png',
    'cifar10-train-1050.png',
    'cifar10-train-1051.png',
    'cifar10-train-1052.png',
    'cifar10-train-1053.png',
    'cifar10-train-1054.png',
    'cifar10-train-1055.png',
    'cifar10-train-1057.png',
    'cifar10-train-1059.png',
    'cifar10-train-1060.png',
    'cifar10-train-1061.png',
    'cifar10-train-1064.png',
    'cifar10-train-1069.png',
    'cifar10-train-1078.png',
    'cifar10-train-1085.png',
    'cifar10-train-1087.png',
    'cifar10-train-1088.png',
    'cifar10-train-1089.png',
    'cifar10-train-1090.png',
    'cifar10-train-1092.png',
    'cifar10-train-1095.png',
    'cifar10-train-1098.png',
    'cifar10-train-1100.png',
    'cifar10-train-1103.png',
    'cifar10-train-1104.png',
    'cifar10-train-1105.png',
    'cifar10-train-1110.png',
    'cifar10-train-1115.png',
    'cifar10-train-1117.png',
    'cifar10-train-1118.png',
    'cifar10-train-1120.png',
    'cifar10-train-1123.png',
    'cifar10-train-1126.png',
    'cifar10-train-1133.png',
    'cifar10-train-1134.png',
    'cifar10-train-1137.png',
    'cifar10-train-1143.png',
    'cifar10-train-1144.png',
    'cifar10-train-1145.png',
    'cifar10-train-1153.png',
    'cifar10-train-1154.png',
    'cifar10-train-1156.png',
    'cifar10-train-1157.png',
    'cifar10-train-1160.png',
    'cifar10-train-1161.png',
    'cifar10-train-1162.png',
    'cifar10-train-1163.png',
    'cifar10-train-1165.png',
    'cifar10-train-1167.png',
    'cifar10-train-1169.png',
    'cifar10-train-1170.png',
    'cifar10-train-1171.png',
    'cifar10-train-1173.png',
    'cifar10-train-1175.png',
    'cifar10-train-1184.png',
    'cifar10-train-1186.png',
    'cifar10-train-1188.png',
    'cifar10-train-1190.png',
    'cifar10-train-1191.png',
    'cifar10-train-1192.png',
    'cifar10-train-1196.png',
    'cifar10-train-1197.png',
    'cifar10-train-1198.png',
    'cifar10-train-1200.png',
    'cifar10-train-1204.png',
    'cifar10-train-1207.png',
    'cifar10-train-1208.png',
    'cifar10-train-1213.png',
    'cifar10-train-1215.png',
    'cifar10-train-1216.png',
    'cifar10-train-1217.png',
    'cifar10-train-1220.png',
    'cifar10-train-1221.png',
    'cifar10-train-1222.png',
    'cifar10-train-1224.png',
    'cifar10-train-1225.png',
    'cifar10-train-1230.png',
    'cifar10-train-1232.png',
    'cifar10-train-1233.png',
    'cifar10-train-1235.png',
    'cifar10-train-1237.png',
    'cifar10-train-1242.png',
    'cifar10-train-1243.png',
    'cifar10-train-1244.png',
    'cifar10-train-1247.png',
    'cifar10-train-1250.png',
    'cifar10-train-1256.png',
    'cifar10-train-1265.png',
    'cifar10-train-1273.png',
    'cifar10-train-1277.png',
    'cifar10-train-1278.png',
    'cifar10-train-1279.png',
    'cifar10-train-1283.png',
    'cifar10-train-1284.png'
];

async function getRandImg() {
    const randInd = Math.floor(Math.random() * imgArr.length);
    const image = imgArr[randInd];
    const imagePath = `/assets/img/cinic/${image}`;
    document.getElementById('preview').src = imagePath;
    // selects random image from array and then previews

    // before converting and returning a File object
    const response = await fetch(imagePath);
    const blob = await response.blob();
    return new File([blob], 'imageFromRandom', {type: blob.type});
}
</script> <script>
const labels = ["airplane", "automobile", "bird", "cat", "deer", "dog", "frog", "horse", "ship", "truck"];

// standard JS fetch with cors proxy
async function request(url) {
    const response = await fetch('https://cors-anywhere.herokuapp.com/' + url, {
        method: 'GET',
        headers: {
            'Accept': '*/*'
        }
    });
    if (!response.ok) throw new Error('Error ' + response.status);
    const blob = await response.blob();
    return new File([blob], 'imageFromUrl', {type: blob.type});
}

async function processAndPredict(file) {
    const reader = new FileReader();
    reader.onload = async function(event) {
        const img = new Image();
        img.onload = async function() {
            const canvas = document.createElement('canvas');
            canvas.width = 32;
            canvas.height = 32;

            const ctx = canvas.getContext('2d');
            ctx.drawImage(img, 0, 0, 32, 32); // converts any size to 32x32
            document.getElementById('preview').src = canvas.toDataURL();
            const imageData = ctx.getImageData(0, 0, 32, 32);

            const mean = [0.485, 0.456, 0.406];
            const std  = [0.229, 0.224, 0.225];

            // this loop converts the image to a tensor with normalization
            const inputData = new Float32Array(32 * 32 * 3);
            for (let i = 0; i < 32; i++) {
                for (let j = 0; j < 32; j++) {
                    const pixelIndex = (i * 32 + j) * 4;
                    const r = imageData.data[pixelIndex] / 255.0;
                    const g = imageData.data[pixelIndex + 1] / 255.0;
                    const b = imageData.data[pixelIndex + 2] / 255.0;

                    const idx = i * 32 + j;
                    inputData[idx] = (r - mean[0]) / std[0];
                    inputData[1024 + idx] = (g - mean[1]) / std[1];
                    inputData[2048 + idx] = (b - mean[2]) / std[2];
                }
            }

            // https://www.youtube.com/watch?v=Vs730jsRgO8
            const session = await ort.InferenceSession.create('/assets/models/model.onnx');
            const inputTensor = new ort.Tensor('float32', inputData, [1, 3, 32, 32]);
            const feeds = {'input.1': inputTensor};
            const outputMap = await session.run(feeds);
            const outputTensor = outputMap['498'];
            const tensorData = outputTensor.data;

            function softmax(logits) {
                const highest = Math.max(...logits);
                const shifted = logits.map(score => Math.exp(score - highest));
                const total = shifted.reduce((acc, val) => acc + val, 0);
                return shifted.map(prob => (prob / total) * 100);
            }

            const predictions = softmax(tensorData);

            let maxIndex = 0;
            for (let i = 1; i < predictions.length; i++) {
                if (predictions[i] > predictions[maxIndex]) {
                    maxIndex = i;
                }
            }

            const predictedLabel = labels[maxIndex];
            const predictedConf = predictions[maxIndex].toFixed(2);

            document.getElementById('output').innerHTML = `I am guessing <strong>${predictedLabel}</strong> was your image with ${predictedConf}% confidence!`;

            var data = [{
                type: 'bar',
                x: labels,
                y: predictions
            }];

            Plotly.newPlot('barChart', data, {autosize: true, margin: {t: 0, b: 50, l: 30, r: 10}}, {displayModeBar: false});
        };
        img.src = event.target.result;
    };
    reader.readAsDataURL(file);
}

document.addEventListener("DOMContentLoaded", function () {
    const fileInput = document.getElementById('fileInput');
    const uploadBtn = document.getElementById('uploadBtn');
    const randomBtn = document.getElementById('randImg');

    // by upload from user
    fileInput.addEventListener('change', function(event) {
        const file = event.target.files[0];
        if (file) {
            processAndPredict(file);
        }
    });

    // by upload from website
    uploadBtn.addEventListener('click', async function() {
        const url = document.getElementById('imageUrl').value.trim();
        if (!url) return;
        try {
            const file = await request(url);
            processAndPredict(file);
        }
        catch (error) {
            document.getElementById('output').textContent = 'Upload error: ' + error.message;
        }
    });

    // by random predefined image
    randomBtn.addEventListener('click', async function() {
        const file = await getRandImg();
        processAndPredict(file);
    });
});
</script> <p>Obviously, if you’re here, you’re not just a robot trying to tell the difference between an airplane and a ship. I hope not… Anyways, I know you want to see some actual code. I will try to explain each portion of my code that has resulted in the model, that you should have tried, above.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torchvision</span>
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="n">torchvision.datasets</span> <span class="kn">import</span> <span class="n">ImageFolder</span>
<span class="kn">from</span> <span class="n">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="n">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">models</span>
<span class="kn">from</span> <span class="n">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span>
<span class="kn">from</span> <span class="n">nnFunctions</span> <span class="kn">import</span> <span class="o">*</span>
</code></pre></div></div> <ul> <li> <strong>torchvision.datasets.ImageFolder</strong>: Converts a folder of images (CINIC-10) to a dataset.</li> <li> <strong>torch.utils.data.DataLoader</strong>: Converts a dataset to a dataloader for future usage.</li> <li> <strong>torchvision.datasets</strong>: Library which provides the CIFAR-10 dataset so I don’t have to create more folders.</li> <li> <strong>torchvision.models</strong>: Library which provides the MobileNet-V3L model which is used.</li> <li> <strong>torchvision.transforms</strong>: Library providing image alterations to improve training.</li> <li> <strong>nnFunctions.py</strong>: Python file with both a training and testing loop function.</li> </ul> <p><strong>Note</strong>: As of writing this, I have a moderate understanding of PyTorch but still need to learn to implement machine learning without the help of libraries.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">device</span> <span class="o">=</span> <span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span>
<span class="nf">print</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">trTransforms</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="nc">Compose</span><span class="p">([</span>
    <span class="n">transforms</span><span class="p">.</span><span class="nc">RandomHorizontalFlip</span><span class="p">(),</span>
    <span class="n">transforms</span><span class="p">.</span><span class="nc">RandomVerticalFlip</span><span class="p">(),</span>
    <span class="n">transforms</span><span class="p">.</span><span class="nc">RandomRotation</span><span class="p">(</span><span class="mi">60</span><span class="p">),</span>
    <span class="n">transforms</span><span class="p">.</span><span class="nc">ToTensor</span><span class="p">(),</span>
    <span class="n">transforms</span><span class="p">.</span><span class="nc">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="p">[</span><span class="mf">0.47889522</span><span class="p">,</span> <span class="mf">0.47227842</span><span class="p">,</span> <span class="mf">0.43047404</span><span class="p">],</span> <span class="n">std</span><span class="o">=</span><span class="p">[</span><span class="mf">0.24205776</span><span class="p">,</span> <span class="mf">0.23828046</span><span class="p">,</span> <span class="mf">0.25874835</span><span class="p">])</span>
<span class="p">])</span>

<span class="n">teTransforms</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="nc">Compose</span><span class="p">([</span>
    <span class="n">transforms</span><span class="p">.</span><span class="nc">ToTensor</span><span class="p">(),</span>
    <span class="n">transforms</span><span class="p">.</span><span class="nc">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="p">[</span><span class="mf">0.47889522</span><span class="p">,</span> <span class="mf">0.47227842</span><span class="p">,</span> <span class="mf">0.43047404</span><span class="p">],</span> <span class="n">std</span><span class="o">=</span><span class="p">[</span><span class="mf">0.24205776</span><span class="p">,</span> <span class="mf">0.23828046</span><span class="p">,</span> <span class="mf">0.25874835</span><span class="p">])</span>
<span class="p">])</span>
</code></pre></div></div> <p>These few lines of code are just to set up torch and image alterations for the datasets. The first line will detect if there is an accelerator and will use it. Below that is a training transforms and a testing transforms.</p> <p>Training transform is made up of:</p> <ul> <li>50% chance for a horizontal flip</li> <li>50% chance for a vertical flip</li> <li>±60 degrees rotation</li> </ul> <p>In both the training and testing transforms, the images are eventually converted into tensors and normalized with values I found <a href="https://github.com/BayesWatch/cinic-10" rel="external nofollow noopener" target="_blank">here</a>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">trainData</span> <span class="o">=</span> <span class="nc">ImageFolder</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="sh">"</span><span class="s">train</span><span class="sh">"</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">trTransforms</span><span class="p">,</span> <span class="n">target_transform</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="n">trainDataLoader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">trainData</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">exTrainData</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="nc">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="sh">"</span><span class="s">data</span><span class="sh">"</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">trTransforms</span><span class="p">,</span> <span class="n">target_transform</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="n">exTrainDataLoader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">exTrainData</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">testData</span> <span class="o">=</span> <span class="nc">ImageFolder</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="sh">"</span><span class="s">test</span><span class="sh">"</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">teTransforms</span><span class="p">)</span>
<span class="n">testDataLoader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">testData</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">)</span>

<span class="n">exTestData</span> <span class="o">=</span> <span class="nc">ImageFolder</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="sh">"</span><span class="s">valid</span><span class="sh">"</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">teTransforms</span><span class="p">)</span>
<span class="n">exTestDataLoader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">exTestData</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">)</span>

<span class="n">classesLength</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">trainData</span><span class="p">.</span><span class="n">classes</span><span class="p">)</span>
</code></pre></div></div> <p>Now, this is actually where I load in the data. Note that CINIC-10 has already been downloaded and un-tarred.</p> <ul> <li>torchvision’s ImageFolder will be used to create a dataset from the CINIC-10’s <strong>train</strong> folder with the training transforms and then turned into a training DataLoader.</li> <li>Extra training data will downloaded from torchvision.datasets in the form of CIFAR-10 and will also have the training transforms applied before becoming a DataLoader object.</li> <li>Testing data will be taken from CINIC-10’s <strong>test</strong> and <strong>valid</strong> folder with the testing transforms and then turned into a testing DataLoader.</li> <li>Finally, a variable “classesLength” will be set to the amount of classes (10) for future use</li> </ul> <p>Overall, you might have one question: <em>Why are you training and testing on both datasets?</em> I am not very knowledgeable but I do know that more data equals higher accuracy, so…yeah.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="p">.</span><span class="nf">mobilenet_v3_large</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">models</span><span class="p">.</span><span class="n">MobileNet_V3_Large_Weights</span><span class="p">.</span><span class="n">DEFAULT</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="n">classifier</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
    <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span>
    <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">classifier</span><span class="p">[</span><span class="mi">3</span><span class="p">].</span><span class="n">in_features</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">f</span><span class="o">=</span><span class="sh">'</span><span class="s">model.pth</span><span class="sh">'</span><span class="p">))</span>
</code></pre></div></div> <p>This is where everything actually gets fun. I know that it is possible to create a model like this myself but a pretrained model is avaliable and could be used for my purpose.</p> <p>This is MobileNet V3 Large, which should offer good performance while still being lightweight. It does live up to its promises since it only takes a fractions of a second to process an image on a crappy phone. I also decided to add a dropout layer to prevent overfitting and made sure it outputted 10 values at the end.</p> <p><strong>Note</strong>: I am loading “model.pth” since I was manually changing the learning rate.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">():</span>
    <span class="n">param</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span>

<span class="n">lossFx</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">optim</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">AdamW</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.00003</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">)</span>
</code></pre></div></div> <ul> <li> <strong>model.parameters loop</strong>: Make sure all parameters are allowed to pass through backpropagation.</li> <li> <strong>nn.CrossEntropyLoss</strong>: Loss function that seems to be really effective for multi-classification problems.</li> <li> <strong>torch.optim.AdamW</strong>: Optimizer set with a manual learning rate and a weight_decay.</li> </ul> <p>Since the learning rate is changed every so often when progress slow, here is an example of what I have done to this model.</p> <table style="width: 100%; border-collapse: collapse; text-align: center;"> <thead> <tr style="background-color: #f2f2f2;"> <th style="border: 1px solid #ddd; padding: 8px;">Trial</th> <th style="border: 1px solid #ddd; padding: 8px;">Learning Rate</th> </tr> </thead> <tbody> <tr> <td style="border: 1px solid #ddd; padding: 8px;">1 - 8</td> <td style="border: 1px solid #ddd; padding: 8px;">0.001</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">8 - 15</td> <td style="border: 1px solid #ddd; padding: 8px;">0.0007</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">15 - 20</td> <td style="border: 1px solid #ddd; padding: 8px;">0.0003</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">20 - 25</td> <td style="border: 1px solid #ddd; padding: 8px;">0.00009</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">25 - 32</td> <td style="border: 1px solid #ddd; padding: 8px;">0.00003</td> </tr> </tbody> </table> <p></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">trial</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">7</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Trial {}: </span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">trial</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">end</span><span class="o">=</span><span class="sh">''</span><span class="p">)</span>
    <span class="nf">trainStep</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">trainDataLoader</span><span class="p">,</span> <span class="n">lossFx</span><span class="p">,</span> <span class="n">optim</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
    <span class="nf">testStep</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">testDataLoader</span><span class="p">,</span> <span class="n">lossFx</span><span class="p">,</span> <span class="n">optim</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
    <span class="nf">trainStep</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">exTrainDataLoader</span><span class="p">,</span> <span class="n">lossFx</span><span class="p">,</span> <span class="n">optim</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
    <span class="nf">testStep</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">exTestDataLoader</span><span class="p">,</span> <span class="n">lossFx</span><span class="p">,</span> <span class="n">optim</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</code></pre></div></div> <p>It is pointless to show a loop with functions in them without the code inside the functions. The current trial is printed and each of these functions trains or tests the model.</p> <p>Since the file is somewhat long, I am going to link the file with the <em>trainStep</em> and <em>testStep</em> loops <a href="https://github.com/echen0719/code-suite/blob/main/Machine%20Learning/nnFunctions.py" rel="external nofollow noopener" target="_blank">here</a>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="n">obj</span><span class="o">=</span><span class="n">model</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">(),</span> <span class="n">f</span><span class="o">=</span><span class="sh">'</span><span class="s">model.pth</span><span class="sh">'</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="n">onnx</span><span class="p">.</span><span class="nf">export</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="sh">'</span><span class="s">model.onnx</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>This is prety straightforward. Save the model so I can manually change the learning rate for another training cycle. At the end, I output the model as a ‘.onnx’ file which is what is actually running in the browser.</p> <h4 id="conclusion">Conclusion</h4> <p>Running on validation data from CINIC-10 shows a <strong>75.02%</strong> accuracy and tends to confuse trucks, ships, and automobiles with each other. That is solid, given that it was only trained for 32 epochs. The model might be slightly complex for 32x32 images. In the future, I will try experimenting with different models such as <em>EfficientNet</em> or <em>VGG</em>, or custom models. Overall, a great experience for me to learn more about image classification.</p> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Eric Chen. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.js" integrity="sha256-4rppopQE9POKfukn2kEvhJ9Um25Cf6+IDVkARD0xh78=" crossorigin="anonymous"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-TW5298Q2JL"></script> <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'G-TW5298Q2JL');
  </script> <script defer src="/assets/js/google-analytics-setup.js?12374742c4b1801ba82226e617af7e2d"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>